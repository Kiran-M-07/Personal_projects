{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb245f8-cb9b-4c0b-8706-c41e78fc4a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 09:15:05,158\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.6.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from get_your_ans import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "176c7ef0-dad8-4871-bb99-96202d9b36c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-09 09:17:48 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-09 09:17:48 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-09 09:18:00 config.py:549] This model supports multiple tasks: {'score', 'reward', 'generate', 'embed', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 03-09 09:18:00 config.py:1382] Defaulting to use mp for distributed inference\n",
      "INFO 03-09 09:18:00 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='mistralai/Mistral-7B-Instruct-v0.2', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.2, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Anaconda/lib/python3.10/site-packages/vllm/transformers_utils/tokenizer_group/tokenizer_group.py:25: FutureWarning: It is strongly recommended to run mistral models with `--tokenizer-mode \"mistral\"` to ensure correct encoding and decoding.\n",
      "  self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 03-09 09:18:01 utils.py:2128] CUDA was previously initialized. We must use the `spawn` multiprocessing start method. Setting VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information.\n",
      "WARNING 03-09 09:18:01 multiproc_worker_utils.py:300] Reducing Torch parallelism from 40 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 03-09 09:18:01 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 03-09 09:18:03 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-09 09:18:03 cuda.py:226] Using XFormers backend.\n",
      "INFO 03-09 09:18:09 __init__.py:207] Automatically detected platform cuda.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:09 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:11 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:11 cuda.py:226] Using XFormers backend.\n",
      "INFO 03-09 09:18:12 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:12 utils.py:916] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:12 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 03-09 09:18:12 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 03-09 09:18:12 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:12 custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 03-09 09:18:12 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_cbcd357e'), local_subscribe_port=41475, remote_subscribe_port=None)\n",
      "INFO 03-09 09:18:12 model_runner.py:1110] Starting to load model mistralai/Mistral-7B-Instruct-v0.2...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:12 model_runner.py:1110] Starting to load model mistralai/Mistral-7B-Instruct-v0.2...\n",
      "INFO 03-09 09:18:13 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:13 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08932c5497f2452a9b6f74535d2eca6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-09 09:18:24 model_runner.py:1115] Loading model weights took 6.7545 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:24 model_runner.py:1115] Loading model weights took 6.7545 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:32 worker.py:267] Memory profiling takes 7.60 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:32 worker.py:267] the current vLLM instance can use total_gpu_memory (31.75GiB) x gpu_memory_utilization (0.90) = 28.57GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:32 worker.py:267] model weights take 6.75GiB; non_torch_memory takes 0.32GiB; PyTorch activation peak memory takes 2.07GiB; the rest of the memory reserved for KV Cache is 19.43GiB.\n",
      "INFO 03-09 09:18:32 worker.py:267] Memory profiling takes 7.61 seconds\n",
      "INFO 03-09 09:18:32 worker.py:267] the current vLLM instance can use total_gpu_memory (31.75GiB) x gpu_memory_utilization (0.90) = 28.57GiB\n",
      "INFO 03-09 09:18:32 worker.py:267] model weights take 6.75GiB; non_torch_memory takes 0.29GiB; PyTorch activation peak memory takes 2.06GiB; the rest of the memory reserved for KV Cache is 19.46GiB.\n",
      "INFO 03-09 09:18:32 executor_base.py:111] # cuda blocks: 19897, # CPU blocks: 4096\n",
      "INFO 03-09 09:18:32 executor_base.py:116] Maximum concurrency for 32768 tokens per request: 9.72x\n",
      "INFO 03-09 09:18:35 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:35 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 28/35 [00:22<00:05,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:18:58 custom_all_reduce.py:226] Registering 2275 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 35/35 [00:27<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-09 09:19:03 custom_all_reduce.py:226] Registering 2275 cuda graph addresses\n",
      "INFO 03-09 09:19:03 model_runner.py:1562] Graph capturing finished in 28 secs, took 1.06 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=8488)\u001b[0;0m INFO 03-09 09:19:03 model_runner.py:1562] Graph capturing finished in 28 secs, took 1.06 GiB\n",
      "INFO 03-09 09:19:03 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 39.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Load an open-source LLM with vLLM (change model if needed)\n",
    "qa_model = LLM(model=\"mistralai/Mistral-7B-Instruct-v0.2\", dtype=\"float16\",tensor_parallel_size= 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "655587fe-3d18-4881-b6e1-a63e3c8210cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_vllm(query,pdf_path):\n",
    "    \"\"\"\n",
    "    Generates a response using vLLM based on retrieved chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user’s question.\n",
    "        retrieved_chunks (dict): The retrieved chunks from ChromaDB.\n",
    "\n",
    "    Returns:\n",
    "        str: The LLM-generated response.\n",
    "    \"\"\"\n",
    "    # Combine retrieved chunks as context\n",
    "    retrieved_chunks = your_best_answer(pdf_path,query)\n",
    "    \n",
    "    context = \"\\n\\n\".join(retrieved_chunks[\"documents\"][0])\n",
    "\n",
    "    # Construct the final prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant answering questions based on the given context.\n",
    "    Format your output in a readable manner. Do not generate output that is not part of the context.\n",
    "    Generate brief output is there are no restrictions specified in the question.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Define sampling parameters (adjust for better control)\n",
    "    sampling_params = SamplingParams(max_tokens=512, temperature=0.1, top_p=0.9)\n",
    "\n",
    "    # Generate response using vLLM\n",
    "    outputs = qa_model.generate([prompt], sampling_params)\n",
    "\n",
    "    return outputs[0].outputs[0].text  # Extract the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1a709c9-17eb-407d-96e0-815f1f898045",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './ias_test_file.pdf'\n",
    "query = 'What is discussed about article 110?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a60870b-b94e-4e57-bb20-01a9ed89cc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: chunk_0\n",
      "Add of existing embedding ID: chunk_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 38 embeddings with shape torch.Size([38, 384])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Insert of existing embedding ID: chunk_1\n",
      "Add of existing embedding ID: chunk_1\n",
      "Insert of existing embedding ID: chunk_2\n",
      "Add of existing embedding ID: chunk_2\n",
      "Insert of existing embedding ID: chunk_3\n",
      "Add of existing embedding ID: chunk_3\n",
      "Insert of existing embedding ID: chunk_4\n",
      "Add of existing embedding ID: chunk_4\n",
      "Insert of existing embedding ID: chunk_5\n",
      "Add of existing embedding ID: chunk_5\n",
      "Insert of existing embedding ID: chunk_6\n",
      "Add of existing embedding ID: chunk_6\n",
      "Insert of existing embedding ID: chunk_7\n",
      "Add of existing embedding ID: chunk_7\n",
      "Insert of existing embedding ID: chunk_8\n",
      "Add of existing embedding ID: chunk_8\n",
      "Insert of existing embedding ID: chunk_9\n",
      "Add of existing embedding ID: chunk_9\n",
      "Insert of existing embedding ID: chunk_10\n",
      "Add of existing embedding ID: chunk_10\n",
      "Insert of existing embedding ID: chunk_11\n",
      "Add of existing embedding ID: chunk_11\n",
      "Insert of existing embedding ID: chunk_12\n",
      "Add of existing embedding ID: chunk_12\n",
      "Insert of existing embedding ID: chunk_13\n",
      "Add of existing embedding ID: chunk_13\n",
      "Insert of existing embedding ID: chunk_14\n",
      "Add of existing embedding ID: chunk_14\n",
      "Insert of existing embedding ID: chunk_15\n",
      "Add of existing embedding ID: chunk_15\n",
      "Insert of existing embedding ID: chunk_16\n",
      "Add of existing embedding ID: chunk_16\n",
      "Insert of existing embedding ID: chunk_17\n",
      "Add of existing embedding ID: chunk_17\n",
      "Insert of existing embedding ID: chunk_18\n",
      "Add of existing embedding ID: chunk_18\n",
      "Insert of existing embedding ID: chunk_19\n",
      "Add of existing embedding ID: chunk_19\n",
      "Insert of existing embedding ID: chunk_20\n",
      "Add of existing embedding ID: chunk_20\n",
      "Insert of existing embedding ID: chunk_21\n",
      "Add of existing embedding ID: chunk_21\n",
      "Insert of existing embedding ID: chunk_22\n",
      "Add of existing embedding ID: chunk_22\n",
      "Insert of existing embedding ID: chunk_23\n",
      "Add of existing embedding ID: chunk_23\n",
      "Insert of existing embedding ID: chunk_24\n",
      "Add of existing embedding ID: chunk_24\n",
      "Insert of existing embedding ID: chunk_25\n",
      "Add of existing embedding ID: chunk_25\n",
      "Insert of existing embedding ID: chunk_26\n",
      "Add of existing embedding ID: chunk_26\n",
      "Insert of existing embedding ID: chunk_27\n",
      "Add of existing embedding ID: chunk_27\n",
      "Insert of existing embedding ID: chunk_28\n",
      "Add of existing embedding ID: chunk_28\n",
      "Insert of existing embedding ID: chunk_29\n",
      "Add of existing embedding ID: chunk_29\n",
      "Insert of existing embedding ID: chunk_30\n",
      "Add of existing embedding ID: chunk_30\n",
      "Insert of existing embedding ID: chunk_31\n",
      "Add of existing embedding ID: chunk_31\n",
      "Insert of existing embedding ID: chunk_32\n",
      "Add of existing embedding ID: chunk_32\n",
      "Insert of existing embedding ID: chunk_33\n",
      "Add of existing embedding ID: chunk_33\n",
      "Insert of existing embedding ID: chunk_34\n",
      "Add of existing embedding ID: chunk_34\n",
      "Insert of existing embedding ID: chunk_35\n",
      "Add of existing embedding ID: chunk_35\n",
      "Insert of existing embedding ID: chunk_36\n",
      "Add of existing embedding ID: chunk_36\n",
      "Insert of existing embedding ID: chunk_37\n",
      "Add of existing embedding ID: chunk_37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stored embeddings in ChromaDB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.23s/it, est. speed input: 204.55 toks/s, output: 53.05 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- LLM Response: ----\n",
      "  Article 110 of the Indian Constitution outlines the criteria for a bill to be classified as a Money Bill. Discussions revolve around concerns of misuse of this provision, potential violations, the role of the Speaker in certification, and the need for greater transparency and accountability. Regular reviews and public awareness are also suggested to address these issues. Key opinions include concerns raised by the Supreme Court in the Aadhaar case regarding the potential for the government to bypass the Rajya Sabha and undermine Parliament's bicameral nature.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = generate_response_vllm(query,pdf_path)\n",
    "print(\"---- LLM Response: ----\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d23d11-0c07-40f8-86ca-02cdedbb30a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
